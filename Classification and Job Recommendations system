import tensorflow as tf
import sys
import nltk
import pandas as pd
import io
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, average_precision_score
from sklearn.utils.multiclass import unique_labels
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

data = pd.read_csv(r"C:\Users\algo-\Downloads\clean_jobs.csv")
data['description'] = data['description'].fillna('')

def preprocess(text):
    tokens = nltk.word_tokenize(text)
    print("tokens",tokens)
    tokens = [word.lower() for word in tokens]
    stop_words = set(stopwords.words('english'))
    print("stop word",stopwords)
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

data['processed'] = data['description'].apply(preprocess)
print("data",data)

label_encoder = LabelEncoder()
data['encoded_title'] = label_encoder.fit_transform(data['title'])

train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(train_data['processed'])

train_sequences = tokenizer.texts_to_sequences(train_data['processed'])
train_padded = pad_sequences(train_sequences, maxlen=100)

test_sequences = tokenizer.texts_to_sequences(test_data['processed'])
test_padded = pad_sequences(test_sequences, maxlen=100)

num_classes = len(np.unique(data['encoded_title']))
train_labels = np.array(train_data['encoded_title'])
test_labels = np.array(test_data['encoded_title'])

model_cnn = Sequential()
model_cnn.add(Embedding(input_dim=10000, output_dim=100, input_length=100))
model_cnn.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model_cnn.add(MaxPooling1D(pool_size=2))
model_cnn.add(GlobalMaxPooling1D())
model_cnn.add(Dense(64, activation='relu'))
model_cnn.add(Dense(32, activation='relu'))
model_cnn.add(Dense(num_classes, activation='softmax'))

model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_cnn.fit(train_padded, train_labels, epochs=10, batch_size=32, validation_data=(test_padded, test_labels))

y_pred_probs = model_cnn.predict(test_padded)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = test_labels
used_labels = unique_labels(y_true, y_pred)

print("\nClassification Report:")
print(classification_report(y_true, y_pred, labels=used_labels, target_names=label_encoder.inverse_transform(used_labels), zero_division=0))

precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
print("precision",precision)
recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
print("recall",recall)
f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
print("f1 score",f1)
map_score = average_precision_score(tf.one_hot(y_true, depth=num_classes), y_pred_probs, average='macro')
print("map_score",map_score)

print(f"\nWeighted Precision: {precision:f}")
print(f"Weighted Recall:    {recall:.4f}")
print(f"Weighted F1-Score:  {f1:.4f}")
print(f"Mean Average Precision (MAP): {map_score:.4f}")

############################ RECOMENDATION SYSTEM ############################

user_input = input("\nEnter a job description or your resume summary: ")
print("user input",user_input)
processed_input = preprocess(user_input)
sequence = tokenizer.texts_to_sequences([processed_input])
padded_sequence = pad_sequences(sequence, maxlen=100)
prediction = model_cnn.predict(padded_sequence)
print("prediction",prediction)
predicted_label = np.argmax(prediction)
predicted_title = label_encoder.inverse_transform([predicted_label])[0]
print(f"\nPredicted Job Title: {predicted_title}")

tf_vectorizer = TfidfVectorizer(max_features=5000)
tf_matrix = tf_vectorizer.fit_transform(data['processed'])

user_vec = tf_vectorizer.transform([processed_input])
cosine_similarities = cosine_similarity(user_vec, tf_matrix).flatten()
top_indices = cosine_similarities.argsort()[-5:][::-1]

print("\nTop 5 Recommended Job Titles based on your input:")
for idx in top_indices:
    print(f"\u2192 {data.iloc[idx]['title']} (Score: {cosine_similarities[idx]:.3f})")
